# vLLM复现实现指南

## 项目概述

本指南提供从零开始复现vLLM的完整实现方案，包括完整版和教育版两种实现路径。

## 环境准备

### 1. 系统要求

- **操作系统**: Linux (Ubuntu 20.04+)
- **Python**: 3.8+
- **CUDA**: 11.7+
- **GPU**: NVIDIA GPU with 16GB+ memory
- **内存**: 32GB+ RAM

### 2. 依赖安装

```bash
# 创建虚拟环境
python -m venv vllm-env
source vllm-env/bin/activate

# 安装基础依赖
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers>=4.35.0
pip install flash-attn --no-build-isolation
pip install ray
pip install prometheus-client
pip install fastapi
pip install uvicorn
```

## 完整版vLLM复现方案

### 1. 项目结构

```
vllm-reproduction/
├── vllm/
│   ├── __init__.py
│   ├── config.py
│   ├── engine/
│   │   ├── __init__.py
│   │   ├── llm_engine.py
│   │   └── async_llm_engine.py
│   ├── worker/
│   │   ├── __init__.py
│   │   ├── worker.py
│   │   └── model_runner.py
│   ├── scheduler/
│   │   ├── __init__.py
│   │   ├── scheduler.py
│   │   └── sequence_manager.py
│   ├── attention/
│   │   ├── __init__.py
│   │   ├── paged_attention.py
│   │   └── attention_backend.py
│   ├── core/
│   │   ├── __init__.py
│   │   ├── block_manager.py
│   │   ├── cache_engine.py
│   │   └── sequence.py
│   ├── model_executor/
│   │   ├── __init__.py
│   │   ├── models/
│   │   └── parallel_utils.py
│   ├── sampling/
│   │   ├── __init__.py
│   │   └── sampling_params.py
│   └── utils/
│       ├── __init__.py
│       └── counter.py
├── benchmarks/
├── tests/
├── examples/
└── docs/
```

### 2. 核心组件实现

#### 2.1 配置系统 (config.py)

```python
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any

@dataclass
class ModelConfig:
    """模型配置"""
    model: str
    tokenizer: str
    trust_remote_code: bool = False
    dtype: str = "auto"
    seed: int = 0
    
@dataclass
class CacheConfig:
    """缓存配置"""
    block_size: int = 16
    gpu_memory_utilization: float = 0.9
    num_gpu_blocks: Optional[int] = None
    num_cpu_blocks: Optional[int] = None
    
@dataclass
class ParallelConfig:
    """并行配置"""
    tensor_parallel_size: int = 1
    pipeline_parallel_size: int = 1
    worker_use_ray: bool = False
    
@dataclass
class SchedulerConfig:
    """调度器配置"""
    max_num_seqs: int = 256
    max_num_batched_tokens: int = 8192
    max_model_len: int = 4096
    
@dataclass
class SamplingConfig:
    """采样配置"""
    temperature: float = 1.0
    top_p: float = 1.0
    top_k: int = -1
    max_tokens: int = 2048
```

#### 2.2 序列管理 (core/sequence.py)

```python
from enum import Enum
from typing import List, Optional
import uuid

class SequenceStatus(Enum):
    WAITING = "waiting"
    RUNNING = "running"
    FINISHED = "finished"
    FAILED = "failed"

class Sequence:
    """序列管理类"""
    
    def __init__(self, request_id: str, prompt: str, prompt_token_ids: List[int]):
        self.request_id = request_id
        self.prompt = prompt
        self.prompt_token_ids = prompt_token_ids
        self.token_ids = prompt_token_ids.copy()
        self.status = SequenceStatus.WAITING
        self.block_table: List[int] = []
        self.output_text = ""
        self.output_token_ids: List[int] = []
        
    def append_token(self, token_id: int, token_text: str):
        """添加生成的token"""
        self.output_token_ids.append(token_id)
        self.output_text += token_text
        self.token_ids.append(token_id)
        
    def is_finished(self) -> bool:
        """检查是否完成"""
        return self.status == SequenceStatus.FINISHED
        
    def get_len(self) -> int:
        """获取序列长度"""
        return len(self.token_ids)
```

#### 2.3 块管理器 (core/block_manager.py)

```python
from typing import List, Dict, Optional
import numpy as np

class Block:
    """内存块"""
    
    def __init__(self, block_id: int, block_size: int):
        self.block_id = block_id
        self.block_size = block_size
        self.ref_count = 0
        self.token_ids: List[int] = []
        
    def can_append(self, num_tokens: int) -> bool:
        """检查是否可以添加token"""
        return len(self.token_ids) + num_tokens <= self.block_size
        
    def append(self, token_ids: List[int]):
        """添加token"""
        self.token_ids.extend(token_ids)
        
    def is_full(self) -> bool:
        """检查块是否已满"""
        return len(self.token_ids) >= self.block_size

class BlockAllocator:
    """块分配器"""
    
    def __init__(self, num_blocks: int, block_size: int):
        self.num_blocks = num_blocks
        self.block_size = block_size
        self.free_blocks: List[Block] = []
        self.allocated_blocks: Dict[int, Block] = {}
        
        # 初始化空闲块
        for i in range(num_blocks):
            self.free_blocks.append(Block(i, block_size))
            
    def allocate(self, num_tokens: int) -> List[Block]:
        """分配块"""
        num_blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
        
        if len(self.free_blocks) < num_blocks_needed:
            raise RuntimeError("Not enough free blocks")
            
        allocated = []
        for _ in range(num_blocks_needed):
            block = self.free_blocks.pop()
            block.ref_count = 1
            allocated.append(block)
            self.allocated_blocks[block.block_id] = block
            
        return allocated
        
    def free(self, blocks: List[Block]):
        """释放块"""
        for block in blocks:
            block.ref_count -= 1
            if block.ref_count == 0:
                block.token_ids = []
                self.free_blocks.append(block)
```

#### 2.4 PagedAttention实现 (attention/paged_attention.py)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple

class PagedAttention(nn.Module):
    """PagedAttention实现"""
    
    def __init__(self, num_heads: int, head_dim: int, block_size: int = 16):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.block_size = block_size
        
    def forward(
        self,
        query: torch.Tensor,
        key_cache: torch.Tensor,
        value_cache: torch.Tensor,
        block_tables: torch.Tensor,
        seq_lens: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            query: [num_tokens, num_heads, head_dim]
            key_cache: [num_blocks, block_size, num_heads, head_dim]
            value_cache: [num_blocks, block_size, num_heads, head_dim]
            block_tables: [num_seqs, max_num_blocks]
            seq_lens: [num_seqs]
        """
        batch_size, max_num_blocks = block_tables.shape
        
        # 重塑query以便广播
        query = query.unsqueeze(0)  # [1, num_tokens, num_heads, head_dim]
        
        # 准备KV缓存
        key_cache, value_cache = self._prepare_kv_cache(
            key_cache, value_cache, block_tables, seq_lens
        )
        
        # 计算注意力分数
        attn_scores = torch.matmul(query, key_cache.transpose(-2, -1))
        attn_scores = attn_scores / (self.head_dim ** 0.5)
        
        # 应用注意力掩码
        attn_mask = self._create_attention_mask(seq_lens, query.shape[1])
        attn_scores = attn_scores.masked_fill(attn_mask == 0, -1e9)
        
        # 计算注意力权重
        attn_weights = F.softmax(attn_scores, dim=-1)
        
        # 计算输出
        output = torch.matmul(attn_weights, value_cache)
        output = output.squeeze(0)  # [num_tokens, num_heads, head_dim]
        
        return output
        
    def _prepare_kv_cache(
        self,
        key_cache: torch.Tensor,
        value_cache: torch.Tensor,
        block_tables: torch.Tensor,
        seq_lens: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """准备KV缓存"""
        batch_size, max_num_blocks = block_tables.shape
        num_blocks, block_size, num_heads, head_dim = key_cache.shape
        
        # 根据块表收集KV缓存
        key_cache_gathered = key_cache[block_tables]  # [batch_size, max_num_blocks, block_size, num_heads, head_dim]
        value_cache_gathered = value_cache[block_tables]
        
        # 重塑为 [batch_size, max_seq_len, num_heads, head_dim]
        max_seq_len = max_num_blocks * block_size
        key_cache_reshaped = key_cache_gathered.view(
            batch_size, max_seq_len, num_heads, head_dim
        )
        value_cache_reshaped = value_cache_gathered.view(
            batch_size, max_seq_len, num_heads, head_dim
        )
        
        return key_cache_reshaped, value_cache_reshaped
        
    def _create_attention_mask(self, seq_lens: torch.Tensor, query_len: int) -> torch.Tensor:
        """创建注意力掩码"""
        batch_size = seq_lens.shape[0]
        max_seq_len = seq_lens.max().item()
        
        # 创建因果掩码
        causal_mask = torch.tril(torch.ones(max_seq_len, query_len))
        
        # 创建序列长度掩码
        seq_mask = torch.arange(max_seq_len).expand(batch_size, max_seq_len)
        seq_mask = seq_mask < seq_lens.unsqueeze(1)
        
        # 组合掩码
        mask = causal_mask & seq_mask.unsqueeze(-1)
        
        return mask.float()
```

#### 2.5 调度器实现 (scheduler/scheduler.py)

```python
from typing import List, Dict, Optional
import heapq
from dataclasses import dataclass

@dataclass
class SequenceGroup:
    """序列组"""
    sequences: List[Sequence]
    priority: int = 0
    arrival_time: float = 0.0
    
    def __lt__(self, other):
        if self.priority != other.priority:
            return self.priority > other.priority
        return self.arrival_time < other.arrival_time

class Scheduler:
    """调度器"""
    
    def __init__(self, config):
        self.config = config
        self.waiting: List[SequenceGroup] = []
        self.running: Dict[str, SequenceGroup] = {}
        self.block_allocator = BlockAllocator(
            config.num_gpu_blocks, config.block_size
        )
        
    def add_sequence(self, sequence: Sequence):
        """添加序列"""
        seq_group = SequenceGroup([sequence])
        heapq.heappush(self.waiting, seq_group)
        
    def schedule(self) -> Optional[Dict]:
        """执行调度"""
        # 1. 检查完成的序列
        self._remove_finished_sequences()
        
        # 2. 从等待队列中选择序列
        new_sequences = self._select_new_sequences()
        
        # 3. 分配资源
        if new_sequences:
            allocated = self._allocate_resources(new_sequences)
            if allocated:
                # 将序列移到运行队列
                for seq_group in new_sequences:
                    for seq in seq_group.sequences:
                        self.running[seq.request_id] = seq_group
                        
        # 4. 准备批次
        batch = self._prepare_batch()
        
        return batch if batch else None
        
    def _remove_finished_sequences(self):
        """移除完成的序列"""
        finished_ids = []
        for req_id, seq_group in self.running.items():
            if all(seq.is_finished() for seq in seq_group.sequences):
                finished_ids.append(req_id)
                
        for req_id in finished_ids:
            seq_group = self.running.pop(req_id)
            # 释放资源
            for seq in seq_group.sequences:
                if seq.block_table:
                    blocks = [self.block_allocator.allocated_blocks[block_id] 
                             for block_id in seq.block_table]
                    self.block_allocator.free(blocks)
                    
    def _select_new_sequences(self) -> List[SequenceGroup]:
        """选择新序列"""
        selected = []
        total_tokens = 0
        
        while self.waiting and len(self.running) < self.config.max_num_seqs:
            seq_group = heapq.heappop(self.waiting)
            
            # 检查是否超过批次限制
            seq_len = sum(seq.get_len() for seq in seq_group.sequences)
            if total_tokens + seq_len > self.config.max_num_batched_tokens:
                # 放回队列
                heapq.heappush(self.waiting, seq_group)
                break
                
            selected.append(seq_group)
            total_tokens += seq_len
            
        return selected
        
    def _allocate_resources(self, sequences: List[SequenceGroup]) -> bool:
        """分配资源"""
        try:
            for seq_group in sequences:
                for seq in seq_group.sequences:
                    blocks = self.block_allocator.allocate(seq.get_len())
                    seq.block_table = [block.block_id for block in blocks]
                    
            return True
        except RuntimeError:
            # 资源不足，回滚
            for seq_group in sequences:
                for seq in seq_group.sequences:
                    if seq.block_table:
                        blocks = [self.block_allocator.allocated_blocks[block_id] 
                                 for block_id in seq.block_table]
                        self.block_allocator.free(blocks)
                        seq.block_table = []
            return False
            
    def _prepare_batch(self) -> Optional[Dict]:
        """准备批次"""
        if not self.running:
            return None
            
        sequences = []
        for seq_group in self.running.values():
            sequences.extend(seq_group.sequences)
            
        # 准备批次数据
        batch = {
            'sequences': sequences,
            'input_ids': torch.cat([torch.tensor(seq.token_ids) for seq in sequences]),
            'block_tables': torch.tensor([seq.block_table for seq in sequences]),
            'seq_lens': torch.tensor([seq.get_len() for seq in sequences]),
        }
        
        return batch
```

#### 2.6 模型执行器 (worker/model_runner.py)

```python
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer

class ModelRunner:
    """模型执行器"""
    
    def __init__(self, model_config, device="cuda"):
        self.model_config = model_config
        self.device = device
        
        # 加载模型
        self.model = AutoModelForCausalLM.from_pretrained(
            model_config.model,
            trust_remote_code=model_config.trust_remote_code,
            torch_dtype=torch.float16,
        ).to(device)
        
        self.model.eval()
        
        # 初始化KV缓存
        self.kv_cache = self._init_kv_cache()
        
    def _init_kv_cache(self):
        """初始化KV缓存"""
        config = self.model.config
        num_heads = config.num_attention_heads
        head_dim = config.hidden_size // num_heads
        num_layers = config.num_hidden_layers
        
        # 为每层创建KV缓存
        kv_cache = []
        for _ in range(num_layers):
            key_cache = torch.zeros(
                self.cache_config.num_gpu_blocks,
                self.cache_config.block_size,
                num_heads,
                head_dim,
                device=self.device,
                dtype=torch.float16
            )
            value_cache = torch.zeros(
                self.cache_config.num_gpu_blocks,
                self.cache_config.block_size,
                num_heads,
                head_dim,
                device=self.device,
                dtype=torch.float16
            )
            kv_cache.append((key_cache, value_cache))
            
        return kv_cache
        
    def execute_model(self, batch):
        """执行模型"""
        input_ids = batch['input_ids'].to(self.device)
        block_tables = batch['block_tables'].to(self.device)
        seq_lens = batch['seq_lens'].to(self.device)
        
        # 前向传播
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                past_key_values=self.kv_cache,
                block_tables=block_tables,
                seq_lens=seq_lens,
                use_cache=True
            )
            
        return outputs
```

#### 2.7 引擎实现 (engine/llm_engine.py)

```python
import time
from typing import Dict, List, Optional
import uuid

class LLMEngine:
    """LLM引擎"""
    
    def __init__(self, model_config, cache_config, scheduler_config):
        self.model_config = model_config
        self.cache_config = cache_config
        self.scheduler_config = scheduler_config
        
        # 初始化组件
        self.tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer)
        self.scheduler = Scheduler(scheduler_config)
        self.model_runner = ModelRunner(model_config)
        
        # 请求管理
        self.requests: Dict[str, Dict] = {}
        
    def add_request(
        self,
        prompt: str,
        sampling_params: Dict,
        request_id: Optional[str] = None
    ) -> str:
        """添加请求"""
        if request_id is None:
            request_id = str(uuid.uuid4())
            
        # 分词
        prompt_token_ids = self.tokenizer.encode(prompt)
        
        # 创建序列
        sequence = Sequence(
            request_id=request_id,
            prompt=prompt,
            prompt_token_ids=prompt_token_ids
        )
        
        # 保存请求信息
        self.requests[request_id] = {
            'sequence': sequence,
            'sampling_params': sampling_params,
            'start_time': time.time(),
            'output_text': '',
            'output_tokens': []
        }
        
        # 添加到调度器
        self.scheduler.add_sequence(sequence)
        
        return request_id
        
    def step(self) -> List[Dict]:
        """执行一个推理步骤"""
        # 调度
        batch = self.scheduler.schedule()
        
        if batch is None:
            return []
            
        # 执行模型
        outputs = self.model_runner.execute_model(batch)
        
        # 处理输出
        results = self._process_outputs(batch, outputs)
        
        return results
        
    def _process_outputs(self, batch, outputs) -> List[Dict]:
        """处理模型输出"""
        results = []
        
        # 这里简化处理，实际需要更复杂的逻辑
        for sequence in batch['sequences']:
            if sequence.is_finished():
                result = {
                    'request_id': sequence.request_id,
                    'text': sequence.output_text,
                    'tokens': sequence.output_token_ids,
                    'finish_reason': 'length'
                }
                results.append(result)
                
        return results
        
    def has_unfinished_requests(self) -> bool:
        """检查是否有未完成的请求"""
        return len(self.scheduler.running) > 0 or len(self.scheduler.waiting) > 0
```

### 3. 教育版简化实现

#### 3.1 简化版架构

```
simple_vllm/
├── __init__.py
├── simple_engine.py
├── simple_scheduler.py
├── simple_attention.py
└── simple_cache.py
```

#### 3.2 简化版引擎 (simple_engine.py)

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Dict, Optional
import time

class SimpleLLMEngine:
    """简化版LLM引擎"""
    
    def __init__(self, model_name: str, max_batch_size: int = 4):
        self.model_name = model_name
        self.max_batch_size = max_batch_size
        
        # 加载模型和分词器
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16
        ).cuda()
        
        self.model.eval()
        
        # 请求队列
        self.request_queue: List[Dict] = []
        self.active_requests: Dict[str, Dict] = {}
        
    def add_request(self, prompt: str, max_tokens: int = 100) -> str:
        """添加请求"""
        request_id = f"req_{len(self.request_queue)}"
        
        request = {
            'id': request_id,
            'prompt': prompt,
            'max_tokens': max_tokens,
            'generated_tokens': [],
            'generated_text': '',
            'start_time': time.time()
        }
        
        self.request_queue.append(request)
        return request_id
        
    def generate(self, prompt: str, max_tokens: int = 100) -> str:
        """简单的生成方法"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        return generated_text
        
    def step(self) -> Dict[str, str]:
        """执行一个推理步骤"""
        results = {}
        
        # 处理队列中的请求
        while self.request_queue and len(self.active_requests) < self.max_batch_size:
            request = self.request_queue.pop(0)
            self.active_requests[request['id']] = request
            
        # 批量处理活跃请求
        if self.active_requests:
            batch_results = self._process_batch()
            results.update(batch_results)
            
        return results
        
    def _process_batch(self) -> Dict[str, str]:
        """处理批次"""
        results = {}
        
        for req_id, request in list(self.active_requests.items()):
            if len(request['generated_tokens']) >= request['max_tokens']:
                # 请求完成
                results[req_id] = request['generated_text']
                del self.active_requests[req_id]
            else:
                # 生成下一个token
                next_token = self._generate_next_token(request)
                request['generated_tokens'].append(next_token)
                request['generated_text'] += self.tokenizer.decode([next_token])
                
        return results
        
    def _generate_next_token(self, request: Dict) -> int:
        """生成下一个token"""
        # 构建输入
        prompt_tokens = self.tokenizer.encode(request['prompt'])
        input_tokens = prompt_tokens + request['generated_tokens']
        
        inputs = torch.tensor([input_tokens]).cuda()
        
        with torch.no_grad():
            outputs = self.model(inputs)
            logits = outputs.logits[0, -1, :]
            
            # 简单采样
            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, 1).item()
            
        return next_token
```

#### 3.3 简化版调度器 (simple_scheduler.py)

```python
from typing import List, Dict
import time

class SimpleScheduler:
    """简化版调度器"""
    
    def __init__(self, max_batch_size: int = 4):
        self.max_batch_size = max_batch_size
        self.waiting_requests: List[Dict] = []
        self.active_requests: Dict[str, Dict] = {}
        
    def add_request(self, request: Dict):
        """添加请求"""
        self.waiting_requests.append(request)
        
    def schedule(self) -> List[Dict]:
        """调度请求"""
        # 将等待的请求移到活跃状态
        while self.waiting_requests and len(self.active_requests) < self.max_batch_size:
            request = self.waiting_requests.pop(0)
            request['start_time'] = time.time()
            self.active_requests[request['id']] = request
            
        return list(self.active_requests.values())
        
    def complete_request(self, request_id: str):
        """完成请求"""
        if request_id in self.active_requests:
            request = self.active_requests.pop(request_id)
            request['end_time'] = time.time()
            request['duration'] = request['end_time'] - request['start_time']
            return request
        return None
```

#### 3.4 简化版注意力 (simple_attention.py)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleAttention(nn.Module):
    """简化版注意力机制"""
    
    def __init__(self, embed_dim: int, num_heads: int = 8):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, _ = hidden_states.shape
        
        # 投影到Q, K, V
        q = self.q_proj(hidden_states)
        k = self.k_proj(hidden_states)
        v = self.v_proj(hidden_states)
        
        # 重塑为多头
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 计算注意力
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        
        # 应用注意力
        output = torch.matmul(attn_weights, v)
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, seq_len, self.embed_dim)
        
        return self.out_proj(output)
```

### 4. 测试和验证

#### 4.1 单元测试

```python
# test_simple_engine.py
import unittest
from simple_vllm import SimpleLLMEngine

class TestSimpleEngine(unittest.TestCase):
    
    def setUp(self):
        self.engine = SimpleLLMEngine("gpt2", max_batch_size=2)
        
    def test_add_request(self):
        request_id = self.engine.add_request("Hello, world!")
        self.assertIsNotNone(request_id)
        
    def test_generate(self):
        result = self.engine.generate("The capital of France is")
        self.assertIsInstance(result, str)
        self.assertGreater(len(result), 0)
        
    def test_batch_processing(self):
        # 添加多个请求
        req1 = self.engine.add_request("Hello")
        req2 = self.engine.add_request("World")
        
        # 处理批次
        results = self.engine.step()
        
        # 验证结果
        self.assertIsInstance(results, dict)

if __name__ == "__main__":
    unittest.main()
```

#### 4.2 性能测试

```python
# benchmark.py
import time
import asyncio
from simple_vllm import SimpleLLMEngine

async def benchmark_simple_engine():
    engine = SimpleLLMEngine("gpt2", max_batch_size=4)
    
    # 准备测试数据
    prompts = [
        "The quick brown fox",
        "Hello, how are you?",
        "What is the meaning of life?",
        "In a hole in the ground there lived a hobbit"
    ]
    
    # 添加请求
    request_ids = []
    for prompt in prompts:
        req_id = engine.add_request(prompt, max_tokens=50)
        request_ids.append(req_id)
    
    # 执行推理
    start_time = time.time()
    results = {}
    
    while engine.has_unfinished_requests():
        step_results = engine.step()
        results.update(step_results)
        
    end_time = time.time()
    
    # 输出结果
    print(f"Total time: {end_time - start_time:.2f}s")
    print(f"Throughput: {len(prompts) / (end_time - start_time):.2f} requests/s")
    
    for req_id, result in results.items():
        print(f"Request {req_id}: {result[:100]}...")

if __name__ == "__main__":
    asyncio.run(benchmark_simple_engine())
```

### 5. 部署和使用

#### 5.1 本地部署

```python
# server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from simple_vllm import SimpleLLMEngine
import uvicorn

app = FastAPI()
engine = SimpleLLMEngine("gpt2")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100

class GenerateResponse(BaseModel):
    request_id: str
    text: str
    tokens_generated: int

@app.post("/generate")
async def generate(request: GenerateRequest):
    try:
        request_id = engine.add_request(request.prompt, request.max_tokens)
        
        # 等待生成完成
        while True:
            results = engine.step()
            if request_id in results:
                return GenerateResponse(
                    request_id=request_id,
                    text=results[request_id],
                    tokens_generated=len(engine.active_requests.get(request_id, {}).get('generated_tokens', []))
                )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 5.2 使用示例

```python
# client.py
import requests

def test_client():
    response = requests.post(
        "http://localhost:8000/generate",
        json={"prompt": "Once upon a time", "max_tokens": 50}
    )
    
    if response.status_code == 200:
        result = response.json()
        print(f"Generated text: {result['text']}")
    else:
        print(f"Error: {response.text}")

if __name__ == "__main__":
    test_client()
```

### 6. 进阶优化

#### 6.1 内存优化

```python
# memory_optimization.py
import torch
from typing import Dict, List

class MemoryOptimizer:
    """内存优化器"""
    
    def __init__(self):
        self.gpu_memory = torch.cuda.get_device_properties(0).total_memory
        self.used_memory = 0
        
    def estimate_memory_usage(self, model_config, batch_size: int) -> int:
        """估算内存使用"""
        # 模型权重内存
        model_params = sum(p.numel() for p in model_config.model.parameters())
        model_memory = model_params * 2  # FP16
        
        # KV缓存内存
        kv_cache_memory = self._estimate_kv_cache_memory(model_config, batch_size)
        
        # 激活内存
        activation_memory = self._estimate_activation_memory(model_config, batch_size)
        
        return model_memory + kv_cache_memory + activation_memory
        
    def _estimate_kv_cache_memory(self, model_config, batch_size: int) -> int:
        """估算KV缓存内存"""
        num_layers = model_config.config.num_hidden_layers
        num_heads = model_config.config.num_attention_heads
        head_dim = model_config.config.hidden_size // num_heads
        seq_len = model_config.max_seq_len
        
        # 每层的KV缓存大小
        layer_kv_memory = 2 * batch_size * seq_len * num_heads * head_dim * 2  # bytes
        
        return num_layers * layer_kv_memory
        
    def optimize_batch_size(self, model_config) -> int:
        """优化批次大小"""
        available_memory = self.gpu_memory - self.used_memory
        
        # 二分查找最大批次大小
        low, high = 1, 32
        best_batch_size = 1
        
        while low <= high:
            mid = (low + high) // 2
            memory_needed = self.estimate_memory_usage(model_config, mid)
            
            if memory_needed <= available_memory:
                best_batch_size = mid
                low = mid + 1
            else:
                high = mid - 1
                
        return best_batch_size
```

#### 6.2 性能监控

```python
# performance_monitor.py
import time
import psutil
import torch
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    """性能指标"""
    throughput: float
    latency: float
    gpu_utilization: float
    memory_usage: float
    cache_hit_rate: float

class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.start_time = time.time()
        
    def record_metrics(self, batch_size: int, batch_time: float) -> PerformanceMetrics:
        """记录性能指标"""
        # 计算吞吐量
        throughput = batch_size / batch_time
        
        # 获取GPU利用率
        gpu_utilization = self._get_gpu_utilization()
        
        # 获取内存使用
        memory_usage = self._get_memory_usage()
        
        # 计算缓存命中率（简化版）
        cache_hit_rate = 0.8  # 假设值
        
        metrics = PerformanceMetrics(
            throughput=throughput,
            latency=batch_time,
            gpu_utilization=gpu_utilization,
            memory_usage=memory_usage,
            cache_hit_rate=cache_hit_rate
        )
        
        self.metrics_history.append(metrics)
        return metrics
        
    def _get_gpu_utilization(self) -> float:
        """获取GPU利用率"""
        try:
            return torch.cuda.utilization() / 100.0
        except:
            return 0.0
            
    def _get_memory_usage(self) -> float:
        """获取内存使用率"""
        try:
            return torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory
        except:
            return 0.0
            
    def get_summary(self) -> Dict:
        """获取性能摘要"""
        if not self.metrics_history:
            return {}
            
        avg_throughput = sum(m.throughput for m in self.metrics_history) / len(self.metrics_history)
        avg_latency = sum(m.latency for m in self.metrics_history) / len(self.metrics_history)
        avg_gpu_util = sum(m.gpu_utilization for m in self.metrics_history) / len(self.metrics_history)
        
        return {
            'avg_throughput': avg_throughput,
            'avg_latency': avg_latency,
            'avg_gpu_utilization': avg_gpu_util,
            'total_requests': len(self.metrics_history),
            'total_time': time.time() - self.start_time
        }
```

### 7. 总结

本指南提供了从零开始复现vLLM的完整方案，包括：

1. **完整版实现**：包含PagedAttention、Continuous Batching等核心特性
2. **教育版实现**：简化版本，便于学习和理解核心概念
3. **测试框架**：单元测试和性能测试
4. **部署方案**：本地服务和API接口
5. **优化策略**：内存优化和性能监控

通过按照本指南逐步实现，你可以深入理解vLLM的设计原理和实现细节，为进一步的研究和开发打下坚实基础。